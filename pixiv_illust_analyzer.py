# Pixivã‚¿ã‚°å…±èµ·è§£æGUIãƒ„ãƒ¼ãƒ«ï¼ˆæ¤œç´¢æ–¹å¼é¸æŠæ©Ÿèƒ½ä»˜ãï¼‰
import streamlit as st

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from pixivpy3 import AppPixivAPI
from collections import Counter
import time
import matplotlib.pyplot as plt
import matplotlib

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import platform
system = platform.system()

if system == "Windows":
    matplotlib.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'DejaVu Sans']
elif system == "Darwin":  # macOS
    matplotlib.rcParams['font.family'] = ['Hiragino Sans', 'Yu Gothic', 'DejaVu Sans']
else:  # Linux
    matplotlib.rcParams['font.family'] = ['Noto Sans CJK JP', 'DejaVu Sans']

matplotlib.font_manager._load_fontmanager(try_read_cache=False)

# çŠ¶æ…‹åˆæœŸåŒ–
def get_pixiv_api():
    if 'api' not in st.session_state:
        st.session_state.api = None
    return st.session_state.api

# ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†
def pixiv_login(refresh_token):
    if not refresh_token:
        st.error("refresh_tokenã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        return
    
    api = AppPixivAPI()
    try:
        with st.spinner("Pixivã«ãƒ­ã‚°ã‚¤ãƒ³ä¸­..."):
            api.auth(refresh_token=refresh_token)
            st.session_state.api = api
            st.session_state.logged_in = True
            st.success("âœ… Pixivã«ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸï¼")
    except Exception as e:
        st.session_state.api = None
        st.session_state.logged_in = False
        st.error(f"âŒ Pixivãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        st.info("refresh_tokenãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# è‹±èªã‚¿ã‚°åˆ¤å®šã¨é™¤å¤–æ©Ÿèƒ½
def is_english_tag(tag):
    """ã‚¿ã‚°ãŒè‹±èªã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    import re
    
    # ç©ºæ–‡å­—ã‚„çŸ­ã™ãã‚‹ã‚¿ã‚°ã¯é™¤å¤–ã—ãªã„
    if not tag or len(tag.strip()) < 2:
        return False
    
    tag = tag.strip()
    
    # æ—¥æœ¬èªæ–‡å­—ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°æ—¥æœ¬èªã‚¿ã‚°ã¨ã—ã¦æ‰±ã†
    japanese_chars = re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', tag)
    if japanese_chars:
        return False
    
    # æ•°å­—ã®ã¿ã®å ´åˆã¯é™¤å¤–ã—ãªã„ï¼ˆå¹´å·ãªã©ï¼‰
    if tag.isdigit():
        return False
    
    # è‹±èªã®æ–‡å­—ãŒ80%ä»¥ä¸Šã‚’å ã‚ã‚‹å ´åˆã¯è‹±èªã‚¿ã‚°ã¨ã—ã¦åˆ¤å®š
    english_chars = re.findall(r'[a-zA-Z]', tag)
    total_meaningful_chars = re.findall(r'[a-zA-Z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', tag)
    
    if len(total_meaningful_chars) == 0:
        return False
    
    english_ratio = len(english_chars) / len(total_meaningful_chars)
    return english_ratio >= 0.8

def filter_tags_by_language(tags, exclude_english=True):
    """è¨€èªè¨­å®šã«åŸºã¥ã„ã¦ã‚¿ã‚°ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if not exclude_english:
        return tags, 0
    
    filtered_tags = []
    english_count = 0
    
    for tag in tags:
        if is_english_tag(tag):
            english_count += 1
        else:
            filtered_tags.append(tag)
    
    return filtered_tags, english_count

# AIç”»åƒåˆ¤å®šæ©Ÿèƒ½
def is_ai_generated(illust):
    """ã‚¤ãƒ©ã‚¹ãƒˆãŒAIç”Ÿæˆã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    try:
        # ã‚¤ãƒ©ã‚¹ãƒˆã®æƒ…å ±ã‹ã‚‰AIé–¢é€£ã®æ‰‹ãŒã‹ã‚Šã‚’æ¢ã™
        if hasattr(illust, 'illust_ai_type') and illust.illust_ai_type == 2:
            return True
        
        # ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã§ã®åˆ¤å®š
        if hasattr(illust, 'tags'):
            ai_keywords = ['ai', 'aiç”Ÿæˆ', 'aiã‚¤ãƒ©ã‚¹ãƒˆ', 'stable diffusion', 'midjourney', 'novel ai', 'nai']
            for tag in illust.tags:
                if hasattr(tag, 'name'):
                    tag_name = tag.name.lower()
                    if any(keyword in tag_name for keyword in ai_keywords):
                        return True
                if hasattr(tag, 'translated_name') and tag.translated_name:
                    tag_trans = tag.translated_name.lower()
                    if any(keyword in tag_trans for keyword in ai_keywords):
                        return True
        
        return False
    except:
        return False

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹é–¢æ•°ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰
def get_request_interval(max_illusts):
    """å–å¾—ä»¶æ•°ã«å¿œã˜ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’èª¿æ•´ï¼ˆæœ€ä½1.5ç§’ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰"""
    import random
    
    base_interval = 1.5  # æœ€ä½1.5ç§’
    if max_illusts <= 100:
        base_interval = 1.5  # 100ä»¶ä»¥ä¸‹: 1.5ç§’
    elif max_illusts <= 300:
        base_interval = 2.0  # 300ä»¶ä»¥ä¸‹: 2ç§’
    elif max_illusts <= 500:
        base_interval = 2.5  # 500ä»¶ä»¥ä¸‹: 2.5ç§’
    elif max_illusts <= 750:
        base_interval = 3.0  # 750ä»¶ä»¥ä¸‹: 3ç§’
    else:
        base_interval = 3.5  # 1000ä»¶: 3.5ç§’
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼è¿½åŠ ï¼ˆÂ±20%ã®ç¯„å›²ï¼‰
    jitter = random.uniform(-0.2, 0.2) * base_interval
    final_interval = max(1.5, base_interval + jitter)  # æœ€ä½1.5ç§’ã‚’ä¿è¨¼
    
    return final_interval

# ã‚¨ãƒ©ãƒ¼æ™‚ã®æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•æ©Ÿèƒ½
def exponential_backoff_request(api, request_func, max_retries=3, base_delay=2.0):
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã¨Retry-Afterå°Šé‡æ©Ÿèƒ½ä»˜ãã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    import random
    import time
    from requests.exceptions import RequestException
    
    for attempt in range(max_retries + 1):
        try:
            result = request_func()
            return result, None  # æˆåŠŸæ™‚ã¯ã‚¨ãƒ©ãƒ¼ãªã—
            
        except Exception as e:
            error_message = str(e).lower()
            
            # æœ€å¾Œã®è©¦è¡Œã®å ´åˆã¯è«¦ã‚ã‚‹
            if attempt == max_retries:
                return None, f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°({max_retries})ã«é”ã—ã¾ã—ãŸ: {str(e)}"
            
            # Retry-Afterãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèªï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            retry_after = None
            if hasattr(e, 'response') and e.response and hasattr(e.response, 'headers'):
                retry_after = e.response.headers.get('Retry-After')
            
            # å¾…æ©Ÿæ™‚é–“ã®è¨ˆç®—
            if retry_after:
                try:
                    wait_time = float(retry_after)
                    st.warning(f"â³ ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰Retry-AfteræŒ‡ç¤º: {wait_time}ç§’å¾…æ©Ÿä¸­...")
                except:
                    wait_time = base_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            else:
                # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®ç‰¹åˆ¥å‡¦ç†
            if any(keyword in error_message for keyword in ['rate limit', '429', 'too many requests']):
                wait_time = max(wait_time, 30)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ™‚ã¯æœ€ä½30ç§’
                st.warning(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡ºã€‚{wait_time:.1f}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
            else:
                st.warning(f"ğŸ”„ APIã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€‚{wait_time:.1f}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
            
            time.sleep(wait_time)
    
    return None, "äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼"

def normalize_search_query(query):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–"""
    import re
    query = query.replace('ã€€', ' ').replace('ï¼‹', '+').replace('ï¼Œ', ',')
    query = re.sub(r'[+,|]', ' ', query)
    query = re.sub(r'\s+', ' ', query.strip())
    return query

# R18ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
def detect_r18_content(search_query):
    """R18é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡º"""
    r18_keywords = [
        'r18', 'r-18', 'r_18', 'nsfw', '18ç¦', 'æˆäººå‘ã‘', 'ãˆã£ã¡', 'ã‚¨ãƒ­',
        'nude', 'naked', 'sex', 'hentai', 'ecchi', 'adult', 'å¤§äºº',
        'è£¸', 'ãŠã£ã±ã„', 'å·¨ä¹³', 'ãƒ‘ãƒ³ãƒ„', 'ãƒ‘ãƒ³ãƒãƒ©', 'ã‚»ãƒƒã‚¯ã‚¹', 'ãƒ‰m', 'ãƒ‰s'
    ]
    
    query_lower = search_query.lower()
    for keyword in r18_keywords:
        if keyword in query_lower:
            return True
    return False

# æ¤œç´¢æ–¹å¼ã®èª¬æ˜ã‚’å–å¾—
def get_search_mode_description(search_mode):
    """æ¤œç´¢æ–¹å¼ã®èª¬æ˜ã‚’è¿”ã™"""
    descriptions = {
        "partial_match_for_tags": "ğŸ·ï¸ **ã‚¿ã‚°æ¤œç´¢**ï¼šä½œå“ã«ä»˜ã‘ã‚‰ã‚ŒãŸã‚¿ã‚°ã®ã¿ã‚’æ¤œç´¢å¯¾è±¡ã¨ã—ã¾ã™",
        "exact_match_for_tags": "ğŸ¯ **ã‚¿ã‚°å®Œå…¨ä¸€è‡´**ï¼šã‚¿ã‚°ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‚‚ã®ã®ã¿ã‚’æ¤œç´¢ã—ã¾ã™",
        "title_and_caption": "ğŸ“ **ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡æ¤œç´¢**ï¼šä½œå“ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜æ–‡ã‚’æ¤œç´¢å¯¾è±¡ã¨ã—ã¾ã™",
        "text": "ğŸ” **å…¨æ–‡æ¤œç´¢**ï¼šã‚¿ã‚°ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã™ã¹ã¦ã‚’æ¤œç´¢å¯¾è±¡ã¨ã—ã¾ã™ï¼ˆæœ€ã‚‚å¹…åºƒã„æ¤œç´¢ï¼‰"
    }
    return descriptions.get(search_mode, "ä¸æ˜ãªæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")

# ã‚¿ã‚°åˆ†æï¼ˆæ¤œç´¢æ–¹å¼é¸æŠæ©Ÿèƒ½ä»˜ãï¼‰
def analyze_tags(api, search_query, max_illusts, search_mode="partial_match_for_tags"):
    if not api:
        st.error("APIãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å†ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
        return []
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠ
    debug_container = st.expander("ğŸ” è©³ç´°ãªå‡¦ç†çŠ¶æ³ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼‰", expanded=False)
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’å–å¾—
    request_interval = get_request_interval(max_illusts)
    
    with debug_container:
        st.write("**ğŸ“‹ å‡¦ç†é–‹å§‹æƒ…å ±:**")
        st.write(f"- å…ƒã®æ¤œç´¢ã‚¯ã‚¨ãƒª: `{search_query}`")
        st.write(f"- æ¤œç´¢æ–¹å¼: `{search_mode}`")
        st.write(f"- {get_search_mode_description(search_mode)}")
        st.write(f"- æœ€å¤§å–å¾—ä»¶æ•°: {max_illusts}ä»¶")
        st.write(f"- ãƒ™ãƒ¼ã‚¹ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {request_interval:.1f}ç§’ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãã€æœ€ä½1.5ç§’ä¿è¨¼ï¼‰")
        st.write(f"- ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤: æœ‰åŠ¹ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‹Retry-Afterå°Šé‡ï¼‰")
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–
    normalized_query = normalize_search_query(search_query)
    search_tags = [tag.strip() for tag in normalized_query.split() if tag.strip()]
    
    with debug_container:
        st.write(f"- æ­£è¦åŒ–å¾Œã®ã‚¯ã‚¨ãƒª: `{normalized_query}`")
        st.write(f"- åˆ†å‰²ã•ã‚ŒãŸã‚¿ã‚°: {search_tags}")
        st.write(f"- ã‚¿ã‚°æ•°: {len(search_tags)}")
    
    # R18æ¤œå‡º
    is_r18 = detect_r18_content(search_query)
    if is_r18:
        st.warning("âš ï¸ R18ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™ã€‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
        with debug_container:
            st.write("- R18ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡º: **Yes**")
    else:
        with debug_container:
            st.write("- R18ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡º: No")
    
    all_tags = []
    processed_count = 0
    found_matching_illusts = 0
    api_calls = 0
    ai_filtered_count = 0
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ä¸»è¦ãªã‚¿ã‚°ã‹ã‚‰æ¤œç´¢é–‹å§‹
        search_word = " ".join(search_tags)
        
        with debug_container:
            st.write(f"**ğŸ¯ æ¤œç´¢å®Ÿè¡Œæƒ…å ±:**")
            st.write(f"- æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ã‚¿ã‚°çµåˆï¼‰: `{search_word}`")
            st.write(f"- å…ƒã®åˆ†å‰²ã‚¿ã‚°: {search_tags}")
            st.write(f"- ä½¿ç”¨ã™ã‚‹æ¤œç´¢æ–¹å¼: `{search_mode}`")
        
        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆæ¤œç´¢æ–¹å¼ã‚’é¸æŠå¯èƒ½ã«ï¼‰
        search_params = {
            "word": search_word,
            "search_target": search_mode,  # â† ã“ã“ãŒé¸æŠå¯èƒ½ã«ãªã£ãŸï¼
            "sort": "popular_desc"
        }
        
        # AIç”»åƒé™¤å¤–è¨­å®šã®ç¢ºèª
        exclude_ai = st.session_state.get('exclude_ai', True)
        
        with debug_container:
            st.write(f"- æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {search_params}")
            if exclude_ai:
                st.write(f"- AIç”»åƒé™¤å¤–: **æœ‰åŠ¹** (å¾Œå‡¦ç†ã§åˆ¤å®š)")
            else:
                st.write(f"- AIç”»åƒé™¤å¤–: ç„¡åŠ¹")
        
        next_qs = None
        page_count = 0
        max_pages = max(15, max_illusts // 20)  # æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ã‚’å¢—åŠ 
        
        with debug_container:
            st.write(f"- æœ€å¤§ãƒšãƒ¼ã‚¸æ•°: {max_pages}")
            st.write(f"- äºˆæƒ³å‡¦ç†æ™‚é–“: ç´„{int((max_pages * request_interval) / 60)}åˆ†{int((max_pages * request_interval) % 60)}ç§’")
            st.write("")
            debug_log = st.empty()
        
        while found_matching_illusts < max_illusts and page_count < max_pages:
            # é€²æ—çŠ¶æ³ã‚’ã‚ˆã‚Šè©³ç´°ã«è¡¨ç¤º
            elapsed_time = page_count * request_interval
            estimated_total_time = max_pages * request_interval
            progress_percentage = min((found_matching_illusts / max_illusts) * 100, 100)
            
            status_text.text(f"ğŸ” æ¤œç´¢ä¸­... ãƒšãƒ¼ã‚¸{page_count + 1}/{max_pages} | "
                           f"è©²å½“ä½œå“: {found_matching_illusts}/{max_illusts} ({progress_percentage:.1f}%) | "
                           f"çµŒéæ™‚é–“: {int(elapsed_time//60)}:{int(elapsed_time%60):02d}")
            
            # APIå‘¼ã³å‡ºã—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰
            if next_qs:
                result, error = exponential_backoff_request(
                    api, 
                    lambda: api.search_illust(**next_qs),
                    max_retries=3
                )
            else:
                result, error = exponential_backoff_request(
                    api, 
                    lambda: api.search_illust(**search_params),
                    max_retries=3
                )
            
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†
            if error:
                with debug_container:
                    debug_log.write(f"âŒ ãƒšãƒ¼ã‚¸{page_count + 1}: APIã‚¨ãƒ©ãƒ¼ - {error}")
                st.error(f"APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}")
                break
            
            json_result = result
            api_calls += 1
            
            with debug_container:
                debug_log.write(f"**ãƒšãƒ¼ã‚¸ {page_count + 1} ã®çµæœ:**\n"
                              f"- APIå‘¼ã³å‡ºã—å›æ•°: {api_calls}\n"
                              f"- å–å¾—ã§ããŸä½œå“æ•°: {len(json_result.illusts) if json_result and hasattr(json_result, 'illusts') else 0}\n"
                              f"- å®Ÿéš›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {request_interval:.1f}ç§’ï¼ˆã‚¸ãƒƒã‚¿ãƒ¼è¾¼ã¿ï¼‰")
            
            if not json_result or not hasattr(json_result, 'illusts') or not json_result.illusts:
                with debug_container:
                    debug_log.write(f"âŒ ãƒšãƒ¼ã‚¸{page_count + 1}: æ¤œç´¢çµæœãŒç©ºã§ã™")
                break
            
            page_matching_count = 0
            page_processed_count = 0
            page_ai_filtered = 0
            
            # å„ã‚¤ãƒ©ã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            for illust in json_result.illusts:
                if not hasattr(illust, 'tags'):
                    continue
                
                page_processed_count += 1
                
                # AIç”»åƒã®é™¤å¤–åˆ¤å®š
                if exclude_ai and is_ai_generated(illust):
                    ai_filtered_count += 1
                    page_ai_filtered += 1
                    continue
                
                # ã‚¿ã‚°ãƒªã‚¹ãƒˆã‚’å–å¾—
                illust_tags = []
                for tag in illust.tags:
                    if hasattr(tag, 'name'):
                        illust_tags.append(tag.name)
                    if hasattr(tag, 'translated_name') and tag.translated_name:
                        illust_tags.append(tag.translated_name)
                
                # ã‚¿ã‚°æ¤œç´¢ã®å ´åˆã®ã¿æ¤œç´¢ã‚¿ã‚°ã‚’é™¤å¤–ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å ´åˆã¯é™¤å¤–ã—ãªã„
                if search_mode in ["partial_match_for_tags", "exact_match_for_tags"]:
                    # ã‚¿ã‚°æ¤œç´¢ï¼šæ¤œç´¢ã‚¿ã‚°ã‚’é™¤å¤–ã—ãŸä»–ã®ã‚¿ã‚°ã‚’åé›†
                    filtered_tags = []
                    for tag in illust_tags:
                        tag_lower = tag.lower()
                        is_search_tag = False
                        
                        # æ¤œç´¢ã‚¿ã‚°ã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        for search_tag in search_tags:
                            search_tag_lower = search_tag.lower()
                            if (tag_lower == search_tag_lower or
                                search_tag_lower in tag_lower or 
                                tag_lower in search_tag_lower):
                                is_search_tag = True
                                break
                        
                        if not is_search_tag:
                            filtered_tags.append(tag)
                else:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼šå…¨ã¦ã®ã‚¿ã‚°ã‚’åé›†ï¼ˆæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å«ã‚€ï¼‰
                    filtered_tags = illust_tags
                
                # è¨€èªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
                exclude_english = st.session_state.get('exclude_english', True)
                english_count = 0
                if exclude_english:
                    filtered_tags, english_count = filter_tags_by_language(filtered_tags, exclude_english)
                
                all_tags.extend(filtered_tags)
                found_matching_illusts += 1
                page_matching_count += 1
                
                # æœ€åˆã®æ•°ä»¶ã¯è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º
                if found_matching_illusts <= 5:
                    with debug_container:
                        debug_log.write(f"âœ… ä½œå“ {found_matching_illusts}: "
                                      f"åé›†ã‚¿ã‚°: {len(filtered_tags)}")
                        if exclude_english and english_count > 0:
                            debug_log.write(f"  - è‹±èªã‚¿ã‚°é™¤å¤–: {english_count}ä»¶")
                        # ä½œå“ã®ã‚¿ã‚°ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                        debug_log.write(f"  - ä½œå“ã®ã‚¿ã‚°ä¾‹: {illust_tags[:5]}")
                
                processed_count += 1
                if found_matching_illusts >= max_illusts:
                    break
            
            with debug_container:
                debug_log.write(f"- ã“ã®ãƒšãƒ¼ã‚¸ã®è©²å½“ä½œå“: {page_matching_count}/{page_processed_count}")
                if exclude_ai and page_ai_filtered > 0:
                    debug_log.write(f"- ã“ã®ãƒšãƒ¼ã‚¸ã®AIä½œå“é™¤å¤–: {page_ai_filtered}ä»¶")
                debug_log.write(f"- ç´¯è¨ˆè©²å½“ä½œå“: {found_matching_illusts}, ç´¯è¨ˆåé›†ã‚¿ã‚°: {len(all_tags)}")
            
            progress_bar.progress(min(found_matching_illusts / max_illusts, 1.0))
            
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
            next_qs = api.parse_qs(json_result.next_url) if hasattr(json_result, 'next_url') and json_result.next_url else None
            if not next_qs:
                with debug_container:
                    debug_log.write("â„¹ï¸ æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆæ¤œç´¢çµ‚äº†ï¼‰")
                break
            
            page_count += 1
            
            # å‹•çš„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã§ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰
            if page_count < max_pages and found_matching_illusts < max_illusts:
                # æ¯å›æ–°ã—ã„ã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãé–“éš”ã‚’å–å¾—
                current_interval = get_request_interval(max_illusts)
                time.sleep(current_interval)
                
                # å¤§é‡å–å¾—æ™‚ã®è¿½åŠ ã®é…æ…®
                if max_illusts >= 500 and page_count % 10 == 0:
                    # 10ãƒšãƒ¼ã‚¸ã”ã¨ã«å°‘ã—é•·ã‚ã®ä¼‘æ†©ï¼ˆã‚¸ãƒƒã‚¿ãƒ¼ä»˜ãï¼‰
                    import random
                    extra_wait = 3.0 + random.uniform(0, 2.0)
                    with debug_container:
                        debug_log.write(f"â±ï¸ 10ãƒšãƒ¼ã‚¸å‡¦ç†å®Œäº†ã€ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚è¿½åŠ ä¼‘æ†©ä¸­...({extra_wait:.1f}ç§’)")
                    time.sleep(extra_wait)
    
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        with debug_container:
            st.write(f"**âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°:**")
            st.write(f"- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}")
            st.write(f"- ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ç‚¹ã§ã®å‡¦ç†æ¸ˆã¿ä½œå“æ•°: {processed_count}")
            st.write(f"- ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ç‚¹ã§ã®è©²å½“ä½œå“æ•°: {found_matching_illusts}")
            st.write(f"- APIå‘¼ã³å‡ºã—å›æ•°: {api_calls}")
        return []
    
    finally:
        progress_bar.empty()
        status_text.empty()
    
    # æœ€çµ‚çµæœã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    with debug_container:
        st.write(f"**ğŸ“Š æœ€çµ‚çµæœ:**")
        st.write(f"- ç·å‡¦ç†ä½œå“æ•°: {processed_count}")
        st.write(f"- è©²å½“ä½œå“æ•°: {found_matching_illusts}")
        st.write(f"- åé›†ã‚¿ã‚°ç·æ•°: {len(all_tags)}")
        st.write(f"- ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚°æ•°: {len(set(all_tags)) if all_tags else 0}")
        st.write(f"- APIå‘¼ã³å‡ºã—å›æ•°: {api_calls}")
        st.write(f"- å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {page_count}")
        st.write(f"- ä½¿ç”¨ã—ãŸæ¤œç´¢æ–¹å¼: `{search_mode}`")
        st.write(f"- å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”: {request_interval:.1f}ç§’ï¼ˆã‚¸ãƒƒã‚¿ãƒ¼è¾¼ã¿ï¼‰")
        st.write(f"- ç·å‡¦ç†æ™‚é–“: ç´„{int((page_count * request_interval) // 60)}åˆ†{int((page_count * request_interval) % 60)}ç§’")
        
        # è¨€èªãƒ»AIç”»åƒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµæœã‚’è¡¨ç¤º
        exclude_english = st.session_state.get('exclude_english', True)
        exclude_ai = st.session_state.get('exclude_ai', True)
        if exclude_english:
            st.write(f"- è‹±èªã‚¿ã‚°é™¤å¤–: **æœ‰åŠ¹**")
        else:
            st.write(f"- è‹±èªã‚¿ã‚°é™¤å¤–: ç„¡åŠ¹")
        if exclude_ai:
            st.write(f"- AIç”»åƒé™¤å¤–: **æœ‰åŠ¹** (é™¤å¤–æ•°: {ai_filtered_count}ä»¶)")
        else:
            st.write(f"- AIç”»åƒé™¤å¤–: ç„¡åŠ¹")
    
    if not all_tags:
        st.warning(f"æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.info(f"ğŸ“Š å‡¦ç†çµæœ: {processed_count}ä½œå“ã‚’ç¢ºèªã—ã€{found_matching_illusts}ä½œå“ãŒæ¡ä»¶ã«è©²å½“ã—ã¾ã—ãŸã€‚")
        if exclude_ai and ai_filtered_count > 0:
            st.info(f"ğŸ¤– AIç”»åƒã‚’{ai_filtered_count}ä»¶é™¤å¤–ã—ã¾ã—ãŸã€‚")
        if found_matching_illusts == 0:
            st.info("ğŸ’¡ **è§£æ±ºã®ãƒ’ãƒ³ãƒˆ**:")
            st.info("- ã‚¿ã‚°åã®ã‚¹ãƒšãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            st.info("- ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚¿ã‚°ã§è©¦ã—ã¦ãã ã•ã„") 
            st.info("- å˜ä¸€ã®ã‚¿ã‚°ã§æ¤œç´¢ã—ã¦ã¿ã¦ãã ã•ã„")
            st.info("- R18ã‚¿ã‚°ã®å ´åˆã€ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            st.info("- æ¤œç´¢æ–¹å¼ã‚’ã€Œå…¨æ–‡æ¤œç´¢ã€ã«å¤‰æ›´ã—ã¦ã¿ã¦ãã ã•ã„")
        return []
    
    result_info = f"âœ… {found_matching_illusts}ä»¶ã®è©²å½“ä½œå“ã‹ã‚‰{len(all_tags)}å€‹ã®ã‚¿ã‚°ã‚’åé›†ã—ã¾ã—ãŸã€‚"
    if exclude_ai and ai_filtered_count > 0:
        result_info += f" (AIç”»åƒ{ai_filtered_count}ä»¶ã‚’é™¤å¤–)"
    st.info(result_info)
    
    counter = Counter(all_tags)
    return counter.most_common(30)

# Pixivæ¤œç´¢URLã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def create_pixiv_search_url(original_query, additional_tag):
    """å…ƒã®æ¤œç´¢ã‚¯ã‚¨ãƒªã¨è¿½åŠ ã‚¿ã‚°ã‚’çµ„ã¿åˆã‚ã›ã¦Pixivæ¤œç´¢URLã‚’ç”Ÿæˆ"""
    import urllib.parse
    
    # å…ƒã®ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–
    normalized_original = normalize_search_query(original_query)
    
    # çµ„ã¿åˆã‚ã›ã‚¯ã‚¨ãƒªã‚’ä½œæˆ
    combined_query = f"{normalized_original} {additional_tag}".strip()
    
    # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded_query = urllib.parse.quote(combined_query)
    
    # Pixivæ¤œç´¢URL
    return f"https://www.pixiv.net/tags/{encoded_query}/artworks"

# ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªã‚¿ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹é–¢æ•°
def create_clickable_tag_table(tag_data, original_query):
    """ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ä»˜ãã®ã‚¿ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    if not tag_data:
        st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    st.subheader("ğŸ“‹ ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªã‚¿ã‚°ä¸€è¦§")
    st.markdown("ğŸ’¡ **ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€å…ƒã®æ¤œç´¢ã‚¿ã‚°ã¨çµ„ã¿åˆã‚ã›ã¦Pixivã§æ¤œç´¢ã§ãã¾ã™**")
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆç¢ºå®Ÿã«å‹•ä½œã™ã‚‹æ–¹æ³•ï¼‰
    import pandas as pd
    
    # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    table_data = []
    urls_data = []  # URLã‚’åˆ¥ã§ç®¡ç†
    
    for i, (tag, count) in enumerate(tag_data[:20], 1):
        # æ¤œç´¢URLã‚’ç”Ÿæˆ
        search_url = create_pixiv_search_url(original_query, tag)
        combined_query = f"{normalize_search_query(original_query)} {tag}"
        
        table_data.append({
            "é †ä½": i,
            "ã‚¿ã‚°å": tag,
            "ä½¿ç”¨å›æ•°": f"{count}å›",
            "çµ„ã¿åˆã‚ã›æ¤œç´¢": f"ğŸ” Pixivã§æ¤œç´¢ ({combined_query})"
        })
        
        urls_data.append({
            "tag": tag,
            "url": search_url,
            "query": combined_query
        })
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¡¨ç¤º
    df = pd.DataFrame(table_data)
    st.dataframe(df, use_container_width=True)
    
    # å€‹åˆ¥ã®ãƒªãƒ³ã‚¯ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
    st.markdown("---")
    st.markdown("**ğŸ”— å€‹åˆ¥æ¤œç´¢ãƒªãƒ³ã‚¯:**")
    
    # 3åˆ—ã§ãƒœã‚¿ãƒ³ã‚’é…ç½®
    cols_per_row = 3
    for i in range(0, len(urls_data), cols_per_row):
        cols = st.columns(cols_per_row)
        for j, col in enumerate(cols):
            if i + j < len(urls_data):
                data = urls_data[i + j]
                with col:
                    # Streamlitã®linkãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨
                    if st.link_button(
                        f"ğŸ” {data['tag']}", 
                        data['url'],
                        help=f"ã€Œ{data['query']}ã€ã§Pixivæ¤œç´¢"
                    ):
                        pass  # ãƒªãƒ³ã‚¯ãƒœã‚¿ãƒ³ãªã®ã§ä½•ã‚‚ã—ãªã„
    
    # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
    st.info("ğŸ”— **ä½¿ã„æ–¹**: ä¸Šè¨˜ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€å…ƒã®æ¤œç´¢ã‚¿ã‚°ã¨ãã®ã‚¿ã‚°ã‚’çµ„ã¿åˆã‚ã›ã¦Pixivã§æ¤œç´¢ã§ãã¾ã™ã€‚æ–°ã—ã„ã‚¿ãƒ–ã§é–‹ãã¾ã™ã€‚")

# å††ã‚°ãƒ©ãƒ•è¡¨ç¤ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
def plot_pie_chart(tag_data, original_query):
    if not tag_data:
        st.warning("è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    try:
        labels, counts = zip(*tag_data[:10])
        
        fig, ax = plt.subplots(figsize=(10, 8))
        colors = plt.cm.Set3(range(len(labels)))
        
        # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã®å®‰å…¨ãªå‡¦ç†
        safe_labels = []
        for label in labels:
            try:
                if any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in label):
                    safe_label = label[:10] + ('...' if len(label) > 10 else '')
                else:
                    safe_label = label
                safe_labels.append(safe_label)
            except:
                safe_labels.append(f"Tag_{len(safe_labels)+1}")
        
        wedges, texts, autotexts = ax.pie(
            counts, 
            labels=safe_labels, 
            autopct='%1.1f%%', 
            startangle=90,
            colors=colors
        )
        
        for text in texts:
            text.set_fontsize(8)
        for autotext in autotexts:
            autotext.set_fontsize(8)
            autotext.set_color('white')
            autotext.set_weight('bold')
        
        ax.set_title(f"Tag Usage Frequency (Top {len(labels)})", fontsize=14, pad=20)
        
        st.pyplot(fig)
        plt.close(fig)
        
    except Exception as e:
        st.error(f"ã‚°ãƒ©ãƒ•ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªãƒªã‚¹ãƒˆè¡¨ç¤º
        for i, (tag, count) in enumerate(tag_data[:15], 1):
            st.write(f"{i}. {tag}: {count}å›")

# ãƒ¡ã‚¤ãƒ³GUI
st.set_page_config(
    page_title="Pixiv ã‚¿ã‚°å…±èµ·åˆ†æãƒ„ãƒ¼ãƒ«", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

st.title("ğŸ¨ Pixiv ã‚¿ã‚°å…±èµ·åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæ¤œç´¢æ–¹å¼é¸æŠæ©Ÿèƒ½ä»˜ãï¼‰")
st.markdown("**è¤‡æ•°ã‚¿ã‚°ã®çµ„ã¿åˆã‚ã›ã§ã€ä¸€ç·’ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚¿ã‚°ã‚’åˆ†æã—ã¾ã™**")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False

# ä½¿ã„æ–¹èª¬æ˜
with st.expander("ğŸ“– ä½¿ã„æ–¹ã¨æ–°æ©Ÿèƒ½ï¼ˆæ¤œç´¢æ–¹å¼é¸æŠï¼‰"):
    st.markdown("""
    **ğŸ†• æ–°æ©Ÿèƒ½: æ¤œç´¢æ–¹å¼é¸æŠ**:
    - **ã‚¿ã‚°æ¤œç´¢**: ä½œå“ã«ä»˜ã‘ã‚‰ã‚ŒãŸã‚¿ã‚°ã®ã¿ã‚’æ¤œç´¢ï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
    - **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢**: ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã‚‚å«ã‚ã¦æ¤œç´¢ï¼ˆPixivã‚¦ã‚§ãƒ–ã«è¿‘ã„ï¼‰
    - **å…¨æ–‡æ¤œç´¢**: ã‚¿ã‚°ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ã™ã¹ã¦ã‚’æ¤œç´¢ï¼ˆæœ€ã‚‚å¹…åºƒã„ï¼‰
    - **ã‚¿ã‚°å®Œå…¨ä¸€è‡´**: ã‚¿ã‚°ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‚‚ã®ã®ã¿ï¼ˆæœ€ã‚‚å³å¯†ï¼‰
    
    **ğŸ”§ ä¿®æ­£å†…å®¹**:
    - **æ¤œç´¢æ–¹å¼ã‚’é¸æŠå¯èƒ½ã«**: ã‚¿ã‚°æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’é¸ã¹ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸ
    - **ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªæ¤œç´¢ãƒªãƒ³ã‚¯æ©Ÿèƒ½**: ã‚¿ã‚°ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦çµ„ã¿åˆã‚ã›æ¤œç´¢ãŒå¯èƒ½
    - AIç”»åƒã®åˆ¤å®šã‚’å¾Œå‡¦ç†ã§å®Ÿè¡Œï¼ˆã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã§ã®åˆ¤å®šï¼‰
    - è¤‡æ•°ã‚¿ã‚°æ¤œç´¢ã®è«–ç†ã‚’æ”¹å–„
    - R18ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œç´¢å‡¦ç†ã‚’æœ€é©åŒ–
    - **è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ **
    
    **ğŸ“ ä½¿ç”¨æ–¹æ³•**:
    1. **refresh_token ã‚’å–å¾—**: Pixivã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã‹ã‚‰å–å¾—
    2. **ãƒ­ã‚°ã‚¤ãƒ³**: ä¸Šè¨˜ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™
    3. **ğŸ†• æ¤œç´¢æ–¹å¼ã‚’é¸æŠ**: ã‚¿ã‚°æ¤œç´¢ã‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‹ã‚’é¸ã¶
    4. **æ¤œç´¢**: è¤‡æ•°ã‚¿ã‚°ã®å ´åˆã¯ `ã‚¿ã‚°A ã‚¿ã‚°B` ã®å½¢å¼ã§å…¥åŠ›
    5. **åˆ†æé–‹å§‹**: ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦çµæœã‚’å¾…ã¤
    6. **ğŸ†• çµ„ã¿åˆã‚ã›æ¤œç´¢**: çµæœã®ã‚¿ã‚°ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…ƒã®ã‚¿ã‚°ã¨çµ„ã¿åˆã‚ã›æ¤œç´¢ï¼
    
    **ğŸ” æ¤œç´¢æ–¹å¼ã®é•ã„**:
    
    | æ¤œç´¢æ–¹å¼ | æ¤œç´¢å¯¾è±¡ | ç‰¹å¾´ | ãŠã™ã™ã‚ç”¨é€” |
    |----------|----------|------|-------------|
    | **ã‚¿ã‚°æ¤œç´¢** | ã‚¿ã‚°ã®ã¿ | å¾“æ¥ã®æ–¹å¼ã€ç²¾åº¦é«˜ã„ | ã‚¿ã‚°ã®å…±èµ·é–¢ä¿‚ã‚’èª¿ã¹ãŸã„ |
    | **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢** | ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ | ä½œå“ã®å†…å®¹ã‚’åæ˜  | ç‰¹å®šã®ãƒ†ãƒ¼ãƒãƒ»ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ¢ã™ |
    | **å…¨æ–‡æ¤œç´¢** | ã‚¿ã‚°ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ | æœ€ã‚‚å¹…åºƒã„æ¤œç´¢ | ã§ãã‚‹ã ã‘å¤šãã®ä½œå“ã‚’è¦‹ã¤ã‘ãŸã„ |
    | **ã‚¿ã‚°å®Œå…¨ä¸€è‡´** | ã‚¿ã‚°ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰ | æœ€ã‚‚å³å¯† | æ­£ç¢ºãªã‚¿ã‚°åã§çµã‚Šè¾¼ã¿ãŸã„ |
    
    **ğŸ”— æ–°æ©Ÿèƒ½: ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªæ¤œç´¢ãƒªãƒ³ã‚¯**:
    - åˆ†æçµæœã®ã‚¿ã‚°ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€è‡ªå‹•ã§Pixivæ¤œç´¢ç”»é¢ã«ç§»å‹•
    - å…ƒã®æ¤œç´¢ã‚¿ã‚°ã¨çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢ãŒå®Ÿè¡Œã•ã‚Œã¾ã™
    - ä¾‹: ã€ŒãŠã£ã±ã„ã€ã§åˆ†æâ†’ã€Œã‚ªãƒªã‚¸ãƒŠãƒ«ã€ã‚’ã‚¯ãƒªãƒƒã‚¯â†’ã€ŒãŠã£ã±ã„ ã‚ªãƒªã‚¸ãƒŠãƒ«ã€ã§æ¤œç´¢
    - æ–°ã—ã„ã‚¿ãƒ–ã§é–‹ãã®ã§ã€åˆ†æçµæœã‚’è¦‹ãªãŒã‚‰æ¤œç´¢å¯èƒ½
    
    **ğŸ” ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã«ã¤ã„ã¦**:
    - åˆ†æå®Ÿè¡Œæ™‚ã«ã€Œè©³ç´°ãªå‡¦ç†çŠ¶æ³ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼‰ã€ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
    - ã†ã¾ãå‹•ã‹ãªã„æ™‚ã¯ã€ã“ã®æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
      - ã©ã®ã‚¿ã‚°ã§æ¤œç´¢ã—ã¦ã„ã‚‹ã‹
      - ä½•ä»¶ã®ä½œå“ãŒè¦‹ã¤ã‹ã£ãŸã‹  
      - ã©ã“ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã„ã‚‹ã‹
      - APIå‘¼ã³å‡ºã—ãŒæˆåŠŸã—ã¦ã„ã‚‹ã‹
      - è‹±èªã‚¿ã‚°ãŒä½•ä»¶é™¤å¤–ã•ã‚ŒãŸã‹
      - AIç”»åƒãŒä½•ä»¶é™¤å¤–ã•ã‚ŒãŸã‹
      - ã©ã®æ¤œç´¢æ–¹å¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹
    
    **ğŸ”¤ è‹±èªã‚¿ã‚°é™¤å¤–æ©Ÿèƒ½**:
    - æ—¥æœ¬èªï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ï¼‰ã‚’å«ã‚€ã‚¿ã‚°ã®ã¿ã‚’è¡¨ç¤º
    - ã€Œanimeã€ã€Œcuteã€ã€Œgirlã€ãªã©ã®è‹±èªã‚¿ã‚°ã‚’è‡ªå‹•ã§é™¤å¤–
    - ã‚ˆã‚Šæ—¥æœ¬ã®Pixivãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é–¢é€£æ€§ã®é«˜ã„çµæœãŒå¾—ã‚‰ã‚Œã¾ã™
    
    **ğŸ¤– AIç”»åƒé™¤å¤–æ©Ÿèƒ½ï¼ˆä¿®æ­£ç‰ˆï¼‰**:
    - ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã§AIç”Ÿæˆç”»åƒã‚’åˆ¤å®šãƒ»é™¤å¤–
    - ã€Œaiã€ã€Œaiç”Ÿæˆã€ã€Œstable diffusionã€ãªã©ã®ã‚¿ã‚°ã‚’æŒã¤ä½œå“ã‚’é™¤å¤–
    - ã‚ˆã‚Šäººé–“ãŒæã„ãŸä½œå“ã«ç‰¹åŒ–ã—ãŸã‚¿ã‚°åˆ†æãŒå¯èƒ½
    - APIåˆ¶é™ã‚’å›é¿ã™ã‚‹ãŸã‚å¾Œå‡¦ç†ã§åˆ¤å®š
    
    **ğŸ’¡ æ¤œç´¢ã®ã‚³ãƒ„**:
    - **æ¤œç´¢ä»¶æ•°ã‚’å¢—ã‚„ã—ãŸã„å ´åˆ**: ã€Œå…¨æ–‡æ¤œç´¢ã€ã‚’é¸æŠ
    - **ç²¾å¯†ãªã‚¿ã‚°é–¢ä¿‚ã‚’èª¿ã¹ãŸã„å ´åˆ**: ã€Œã‚¿ã‚°æ¤œç´¢ã€ã‚’é¸æŠ
    - **ä½œå“ã®å†…å®¹é‡è¦–ã®å ´åˆ**: ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã€ã‚’é¸æŠ
    - **âš ï¸ å¤§é‡å–å¾—æ™‚ã®æ³¨æ„**: 500ä»¶ä»¥ä¸Šã®å–å¾—ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆ15-40åˆ†ç¨‹åº¦ï¼‰
    - **ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›**: æœ€ä½1.5ç§’é–“éš”ï¼‹ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ï¼‹è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    - **ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•å¯¾å¿œ**: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã¨Retry-Afterå°Šé‡ã§ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
    - R18ç³»ã‚¿ã‚°ã¯æœ€åˆã¯å°‘ãªã„å–å¾—æ•°ï¼ˆ30-50ä»¶ï¼‰ã§è©¦ã—ã¦ãã ã•ã„
    - è¤‡æ•°ã‚¿ã‚°ã¯é–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’çµ„ã¿åˆã‚ã›ã¦ãã ã•ã„
    - ã†ã¾ãã„ã‹ãªã„å ´åˆã¯ã€Œãƒ‡ãƒãƒƒã‚°æƒ…å ±ã€ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    
    **ä¾‹**: `ãƒ‰Mãƒ›ã‚¤ãƒ›ã‚¤ R-18` ã‚„ `åˆéŸ³ãƒŸã‚¯ VOCALOID` ã‚„ `çŒ« å¯æ„›ã„`
    """)

# ãƒ­ã‚°ã‚¤ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.subheader("ğŸ” Pixivãƒ­ã‚°ã‚¤ãƒ³")
refresh_token = st.text_input(
    "Pixiv refresh_token", 
    type="password",
    help="Pixivã®é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ã‹ã‚‰å–å¾—ã—ã¦ãã ã•ã„"
)

col1, col2 = st.columns([1, 1])
with col1:
    if st.button("ğŸš€ ãƒ­ã‚°ã‚¤ãƒ³", type="primary"):
        pixiv_login(refresh_token)

with col2:
    if st.session_state.get('logged_in', False):
        st.success("âœ… ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿")
    else:
        st.error("âŒ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦")

# æ¤œç´¢ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.markdown("---")
st.subheader("ğŸ” ã‚¿ã‚°åˆ†æ")

# ğŸ†• æ¤œç´¢æ–¹å¼é¸æŠ
st.markdown("**ğŸ†• æ¤œç´¢æ–¹å¼é¸æŠ**")
search_mode_options = {
    "partial_match_for_tags": "ğŸ·ï¸ ã‚¿ã‚°æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰- å¾“æ¥ã®æ–¹å¼",
    "exact_match_for_tags": "ğŸ¯ ã‚¿ã‚°å®Œå…¨ä¸€è‡´ - ã‚ˆã‚Šå³å¯†",
    "title_and_caption": "ğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ï¼‰- ä½œå“å†…å®¹é‡è¦–",
    "text": "ğŸ” å…¨æ–‡æ¤œç´¢ï¼ˆã‚¿ã‚°ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜æ–‡ï¼‰- æœ€ã‚‚å¹…åºƒã„"
}

search_mode = st.selectbox(
    "æ¤œç´¢æ–¹å¼ã‚’é¸æŠã—ã¦ãã ã•ã„",
    options=list(search_mode_options.keys()),
    format_func=lambda x: search_mode_options[x],
    index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¿ã‚°æ¤œç´¢
    help="æ¤œç´¢å¯¾è±¡ã‚’é¸æŠã§ãã¾ã™ã€‚å…¨æ–‡æ¤œç´¢ã«ã™ã‚‹ã¨Pixivã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®æ¤œç´¢ã«è¿‘ã„çµæœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚"
)

# é¸æŠã—ãŸæ¤œç´¢æ–¹å¼ã®èª¬æ˜ã‚’è¡¨ç¤º
st.info(get_search_mode_description(search_mode))

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
st.markdown("**ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š**")
col_setting1, col_setting2 = st.columns([1, 1])

with col_setting1:
    exclude_english = st.checkbox(
        "ğŸ”¤ è‹±èªã‚¿ã‚°ã‚’é™¤å¤–ã™ã‚‹", 
        value=st.session_state.get('exclude_english', True),
        help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã€çµæœã‹ã‚‰è‹±èªã®ã‚¿ã‚°ã‚’é™¤å¤–ã—ã€æ—¥æœ¬èªã‚¿ã‚°ã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™"
    )
    st.session_state.exclude_english = exclude_english

with col_setting2:
    exclude_ai = st.checkbox(
        "ğŸ¤– AIç”»åƒã‚’é™¤å¤–ã™ã‚‹", 
        value=st.session_state.get('exclude_ai', True),
        help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã€AIé–¢é€£ã‚¿ã‚°ã‚’æŒã¤ä½œå“ã‚’æ¤œç´¢å¯¾è±¡ã‹ã‚‰é™¤å¤–ã—ã¾ã™ï¼ˆã‚¿ã‚°ãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼‰"
    )
    st.session_state.exclude_ai = exclude_ai

# è¨­å®šçŠ¶æ³ã®è¡¨ç¤º
col_status1, col_status2 = st.columns([1, 1])
with col_status1:
    if exclude_english:
        st.success("âœ… æ—¥æœ¬èªã‚¿ã‚°ã®ã¿è¡¨ç¤º")
    else:
        st.info("â„¹ï¸ å…¨è¨€èªã®ã‚¿ã‚°ã‚’è¡¨ç¤º")

with col_status2:
    if exclude_ai:
        st.success("âœ… äººé–“ä½œæˆã®ä½œå“ã®ã¿")
    else:
        st.info("â„¹ï¸ AIç”»åƒã‚‚å«ã‚€")

col1, col2 = st.columns([2, 1])
with col1:
    tag_query = st.text_input(
        "æ¤œç´¢ã‚¿ã‚°ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", 
        value="ãƒ‰Mãƒ›ã‚¤ãƒ›ã‚¤ R-18",
        help="æ¤œç´¢æ–¹å¼ã«å¿œã˜ã¦ã‚¿ã‚°ã¾ãŸã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ã®å ´åˆã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚"
    )

with col2:
    max_count = st.selectbox(
        "æœ€å¤§å–å¾—æ•°",
        options=[30, 50, 100, 200, 300, 500, 750, 1000],
        index=1,
        help="å¤§ããªæ•°å€¤ã»ã©æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚R18é–¢é€£ã¯å°‘ãªã„æ•°ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™"
    )

if st.button("ğŸ“Š åˆ†æé–‹å§‹", type="primary"):
    if not st.session_state.get('logged_in', False):
        st.warning("âš ï¸ å…ˆã«Pixivã¸ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
    elif not tag_query.strip():
        st.warning("âš ï¸ æ¤œç´¢ã‚¿ã‚°ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        api = get_pixiv_api()
        if api:
            st.info(f"ã€{tag_query}ã€ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™...ï¼ˆæ¤œç´¢æ–¹å¼: {search_mode_options[search_mode]}ï¼‰")
            results = analyze_tags(api, tag_query, max_count, search_mode)
            
            if results:
                st.success(f"âœ… åˆ†æå®Œäº†ï¼{len(results)}ä»¶ã®ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                
                # çµæœè¡¨ç¤º
                st.subheader(f"ğŸ“ˆ ã€{tag_query}ã€ã¨ä¸€ç·’ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚¿ã‚°")
                
                # ä½¿ç”¨ã—ãŸæ¤œç´¢æ–¹å¼ã®è¡¨ç¤º
                st.markdown(f"**ä½¿ç”¨ã—ãŸæ¤œç´¢æ–¹å¼**: {search_mode_options[search_mode]}")
                
                # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªã‚¿ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º
                create_clickable_tag_table(results, tag_query)
                
                # å††ã‚°ãƒ©ãƒ•è¡¨ç¤º
                st.subheader("ğŸ¥§ ä½¿ç”¨é »åº¦ã‚°ãƒ©ãƒ•")
                plot_pie_chart(results, tag_query)
            else:
                st.error("âŒ æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.info("ğŸ’¡ ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚¿ã‚°ã‚„ã€å˜ä¸€ã®ã‚¿ã‚°ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
                st.info("ğŸ’¡ æ¤œç´¢æ–¹å¼ã‚’ã€Œå…¨æ–‡æ¤œç´¢ã€ã«å¤‰æ›´ã™ã‚‹ã¨ã€ã‚ˆã‚Šå¤šãã®çµæœãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            st.error("âŒ APIæ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚å†ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("ğŸ›¡ï¸ **ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›å¼·åŒ–ç‰ˆ**: æœ€ä½1.5ç§’é–“éš”ï¼‹ãƒ©ãƒ³ãƒ€ãƒ ã‚¸ãƒƒã‚¿ãƒ¼ï¼‹æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‹Retry-Afterå°Šé‡ã§Pixivã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã„è¨­è¨ˆï¼")
